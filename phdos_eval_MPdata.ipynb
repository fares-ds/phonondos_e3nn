{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch_scatter\n",
    "\n",
    "import e3nn\n",
    "from e3nn import rs, o3\n",
    "from e3nn.point.data_helpers import DataPeriodicNeighbors\n",
    "from e3nn.networks import GatedConvParityNetwork\n",
    "from e3nn.kernel_mod import Kernel\n",
    "from e3nn.point.message_passing import Convolution\n",
    "\n",
    "import pymatgen\n",
    "from pymatgen.core.structure import Structure\n",
    "\n",
    "import time, os\n",
    "import datetime\n",
    "import pickle\n",
    "from mendeleev import element\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import h5py\n",
    "from class_evaluate_MPdata import ComprehensiveEvaluation, AtomEmbeddingAndSumLastLayer\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('models/200803-1018_len51max1000_fwin101ord3_trial_run_full_data.torch',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtomEmbeddingAndSumLastLayer(data['state']['linear.weight'].shape[1], data['state']['linear.weight'].shape[0], GatedConvParityNetwork(**data['model_kwargs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomEmbeddingAndSumLastLayer(\n",
       "  (linear): Linear(in_features=118, out_features=64, bias=True)\n",
       "  (model): GatedConvParityNetwork(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (kernel): Kernel (64x0e -> 32x0e,32x0e,32x1o)\n",
       "        )\n",
       "        (1): GatedBlockParity (32x0e + 32x0e + 32x1o -> 32x0e,32x1o)\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Convolution(\n",
       "          (kernel): Kernel (32x0e,32x1o -> 32x0e,64x0e,32x1e,32x1o)\n",
       "        )\n",
       "        (1): GatedBlockParity (32x0e + 64x0e + 32x1e,32x1o -> 32x0e,32x1e,32x1o)\n",
       "      )\n",
       "      (2): Convolution(\n",
       "        (kernel): Kernel (32x0e,32x1e,32x1o -> 51x0e)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(data['state'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/phdos_e3nn_len51max1000_fwin101ord3.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "phfre = data_dict['phfre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/cif_unique_files.pkl', 'rb') as f: \n",
    "    ciflist_dict = pickle.load(f)\n",
    "\n",
    "cif_name = ciflist_dict.get('cif_name')\n",
    "cif_id = ciflist_dict.get('cif_id')\n",
    "num_sites = ciflist_dict.get('num_sites')\n",
    "\n",
    "cif_name_suc = [cif_name[i] for i in range(len(cif_name)) if num_sites[i] <= 13]\n",
    "cif_id_suc = [cif_id[i] for i in range(len(cif_id)) if num_sites[i] <= 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cif_id_suc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_lst = [273.15, 293.15]\n",
    "h5_file = 'phdos_maxsites13_2020Aug27.h5'\n",
    "skip_ids = [4319, 4334]\n",
    "structures = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new h5py data\n",
      "\n",
      "4347   Calculating mp-1205455          \r"
     ]
    }
   ],
   "source": [
    "for i in [x for x in range(0,len(cif_id_suc)) if x not in skip_ids]:\n",
    "    material_id = cif_id_suc[i]\n",
    "    chunk_evaluation = ComprehensiveEvaluation([cif_name_suc[i]], data['model_kwargs'], cif_path='../data/', chunk_id=material_id)\n",
    "    chunk_evaluation.predict_phdos(chunk_evaluation.data,model,device=device)\n",
    "    chunk_evaluation.cal_heatcap(chunk_evaluation.phdos,phfre.tolist(),T_lst,chunk_evaluation.structures)\n",
    "    structures.append(chunk_evaluation.structures)\n",
    "    if os.path.exists(h5_file):\n",
    "        with h5py.File(h5_file, 'a') as hf:\n",
    "            hf[\"material_id\"].resize((hf[\"material_id\"].shape[0]+np.array([material_id])[None,:].shape[0]),axis=0)\n",
    "            hf[\"material_id\"][-np.array([material_id])[None,:].shape[0]:] = np.array([material_id])[None,:]\n",
    "            hf[\"phdos_max1\"].resize((hf[\"phdos_max1\"].shape[0]+np.array(chunk_evaluation.phdos).shape[0]),axis=0)\n",
    "            hf[\"phdos_max1\"][-np.array(chunk_evaluation.phdos).shape[0]:] = np.array(chunk_evaluation.phdos)\n",
    "            hf[\"phdos_norm\"].resize((hf[\"phdos_norm\"].shape[0]+np.array(chunk_evaluation.phdos_norm).shape[0]),axis=0)\n",
    "            hf[\"phdos_norm\"][-np.array(chunk_evaluation.phdos_norm).shape[0]:] = np.array(chunk_evaluation.phdos_norm)\n",
    "            hf[\"heat_cap_mol\"].resize((hf[\"heat_cap_mol\"].shape[0]+np.array(chunk_evaluation.C_v_mol).shape[0]),axis=0)\n",
    "            hf[\"heat_cap_mol\"][-np.array(chunk_evaluation.C_v_mol).shape[0]:] = np.array(chunk_evaluation.C_v_mol)\n",
    "            hf[\"heat_cap_kg\"].resize((hf[\"heat_cap_kg\"].shape[0]+np.array(chunk_evaluation.C_v_kg).shape[0]),axis=0)\n",
    "            hf[\"heat_cap_kg\"][-np.array(chunk_evaluation.C_v_kg).shape[0]:] = np.array(chunk_evaluation.C_v_kg)\n",
    "            \n",
    "            print(\"{}   Calculating mp-{}          \".format(i, cif_id_suc[i]), end=\"\\r\", flush=True)\n",
    "    else:\n",
    "        with h5py.File(h5_file, 'w') as hf:\n",
    "            hf.create_dataset(\"material_id\", data=np.array([material_id])[None,:],\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,None))\n",
    "            hf.create_dataset(\"phdos_max1\", data=np.array(chunk_evaluation.phdos),\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,None))\n",
    "            hf.create_dataset(\"phdos_norm\", data=np.array(chunk_evaluation.phdos_norm),\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,None))\n",
    "            hf.create_dataset(\"heat_cap_mol\", data=np.array(chunk_evaluation.C_v_mol),\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,None))\n",
    "            hf.create_dataset(\"heat_cap_kg\", data=np.array(chunk_evaluation.C_v_kg),\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,None))\n",
    "            hf.create_dataset(\"phfre\", data=phfre,\n",
    "                              compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None))\n",
    "            print(\"Created new h5py data\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"phdos_maxsites13_2020Aug27.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(structures, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('predictions/'+h5_file, 'r') as hf:\n",
    "    material_id_r = hf['material_id'][:]\n",
    "    phdos_max1_r = hf['phdos_max1'][:]\n",
    "    phdos_norm_r = hf['phdos_norm'][:]\n",
    "    heat_cap_r = hf['heat_cap'][:]\n",
    "    phfre_r = hf['phfre'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 50\n",
    "# cif_name_chunks = [cif_name_suc[i:i+chunk_size] for i in range(0,len(cif_name_suc),chunk_size)]\n",
    "# cif_id_chunks = [cif_id_suc[i:i+chunk_size] for i in range(0,len(cif_id_suc),chunk_size)]\n",
    "\n",
    "# for chunk_id in range(253,len(cif_name_chunks)):\n",
    "# #     print(\"Calculating chunk: {:3d}\\n\".format(chunk_id), end=\"\\r\", flush=True)\n",
    "#     chunk_evaluation = ComprehensiveEvaluation(cif_name_chunks[chunk_id], data['model_kwargs'], cif_path='data/', chunk_id=chunk_id)\n",
    "#     chunk_evaluation.predict_phdos(chunk_evaluation.data,model,device=device)\n",
    "#     # T_lst = np.linspace(5,800,160,endpoint=True)\n",
    "#     T_lst = [273.15, 293.15]\n",
    "#     chunk_evaluation.cal_heatcap(chunk_evaluation.phdos,phfre.tolist(),T_lst,chunk_evaluation.structures)\n",
    "\n",
    "#     with open('predictions/max50sites_chunk_{:03}.pkl'.format(chunk_id), 'wb') as f:\n",
    "#         pickle.dump({'material_id': cif_id_chunks[chunk_id],\n",
    "#                      'phdos_max1': chunk_evaluation.phdos,\n",
    "#                      'phdos_norm': chunk_evaluation.phdos_norm,\n",
    "#                      'T': T_lst,\n",
    "#                      'heat_capacity': chunk_evaluation.C_v}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.arange(chunk_size)\n",
    "# fig = plt.figure(figsize=(18, 10), constrained_layout=True)\n",
    "# outer = gridspec.GridSpec(10, 5, wspace=0.25, hspace=2)\n",
    "# for i in range(chunk_size):\n",
    "#     inner = gridspec.GridSpecFromSubplotSpec(1, 2, wspace=0.5, hspace=0,\n",
    "#                     subplot_spec=outer[i])\n",
    "\n",
    "#     ax0 = plt.Subplot(fig, inner[0])\n",
    "#     ax0.plot(phfre,chunk_evaluation.phdos[i],c='C0')\n",
    "# #     ax0.set_xlabel('Frequency [1/cm]')\n",
    "# #     ax0.set_ylabel('Phonon DOS [a.u.]')\n",
    "#     ax0.set_title('mp-{}: '.format(cif_id_chunks[chunk_id][i])+str(chunk_evaluation.cif_strlist[i][1][5:]))\n",
    "#     fig.add_subplot(ax0)\n",
    "\n",
    "#     ax1 = plt.Subplot(fig, inner[1])\n",
    "#     ax1.plot(T_lst,chunk_evaluation.C_v[i],c='C1')\n",
    "# #     ax1.set_xlabel('T [K]')\n",
    "# #     ax1.set_ylabel('Heat Capacity [J/(mol K)]')\n",
    "#     ax1.set_title('Num Sites:{:3d}'.format(chunk_evaluation.structures[i].num_sites))\n",
    "#     fig.add_subplot(ax1)\n",
    "# fig.suptitle('CIF Chunk: {}'.format(chunk_id), horizontalalignment='center')\n",
    "# # fig.show()\n",
    "# fig.savefig('figs/phdos_Cv_{:03}.png'.format(chunk_id), dpi=300, bbox_inches='tight')\n",
    "# plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
